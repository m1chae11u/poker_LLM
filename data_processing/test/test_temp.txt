(pokerenv) michael_lu@lingua:/data/michael_lu/poker_LLM/litgpt/checkpoints/meta-llama$ CUDA_VISIBLE_DEVICES=1,2,3,4 litgpt finetune_full /data/micha
el_lu/poker_LLM/litgpt/checkpoints/meta-llama/Llama-3.2-3B \                                                                                        
>   --data JSON \                                                                                                                                   
>   --data.json_path /home/michael_lu/poker_LLM/data/formatted_for_lit_gpt/lit_gpt_custom_setup/combined_train_set.json \                           
>   --data.val_split_fraction 0.001 \
>   --out_dir /data/michael_lu/poker_LLM/litgpt/full_finetune_out \
>   --train.save_interval 500 \
>   --train.log_interval 50 \
>   --train.epochs 3 \
>   --train.global_batch_size 8 \
>   --train.micro_batch_size 1 \
>   --eval.interval 200 \
>   --eval.initial_validation False \
>   --devices 4
{'access_token': None,
 'checkpoint_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/checkpoints/meta-llama/Llama-3.2-3B'),
 'data': JSON(json_path=PosixPath('/home/michael_lu/poker_LLM/data/formatted_for_lit_gpt/lit_gpt_custom_setup/combined_train_set.json'),
              mask_prompt=False,
              val_split_fraction=0.001,
              prompt_style=<litgpt.prompts.Alpaca object at 0x7fc361f6d660>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=200,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True,
                  evaluate_example='first'),
 'logger_name': 'csv',
 'num_nodes': 1,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/full_finetune_out'),
 'precision': None,
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=500,
                    log_interval=50,
                    global_batch_size=8,
                    micro_batch_size=1,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=3,
                    max_tokens=None,
                    max_steps=None,
                    max_seq_length=None,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
Warning: Not all GPUs are fully connected via NVLink. Some GPUs are connected via slower interfaces. It is recommended to switch to a different machine with faster GPU connections for optimal multi-GPU training performance.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
{'access_token': None,
 'checkpoint_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/checkpoints/meta-llama/Llama-3.2-3B'),
 'data': JSON(json_path=PosixPath('/home/michael_lu/poker_LLM/data/formatted_for_lit_gpt/lit_gpt_custom_setup/combined_train_set.json'),
              mask_prompt=False,
              val_split_fraction=0.001,
              prompt_style=<litgpt.prompts.Alpaca object at 0x7efba2bff6d0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=200,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True,
                  evaluate_example='first'),
 'logger_name': 'csv',
 'num_nodes': 1,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/full_finetune_out'),
 'precision': None,
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=500,
                    log_interval=50,
                    global_batch_size=8,
                    micro_batch_size=1,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=3,
                    max_tokens=None,
                    max_steps=None,
                    max_seq_length=None,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/checkpoints/meta-llama/Llama-3.2-3B'),
 'data': JSON(json_path=PosixPath('/home/michael_lu/poker_LLM/data/formatted_for_lit_gpt/lit_gpt_custom_setup/combined_train_set.json'),
              mask_prompt=False,
              val_split_fraction=0.001,
              prompt_style=<litgpt.prompts.Alpaca object at 0x7f7d1d080e20>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=200,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True,
                  evaluate_example='first'),
 'logger_name': 'csv',
'num_nodes': 1,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/full_finetune_out'),
 'precision': None,
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=500,
                    log_interval=50,
                    global_batch_size=8,
                    micro_batch_size=1,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=3,
                    max_tokens=None,
                    max_steps=None,
                    max_seq_length=None,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/checkpoints/meta-llama/Llama-3.2-3B'),
 'data': JSON(json_path=PosixPath('/home/michael_lu/poker_LLM/data/formatted_for_lit_gpt/lit_gpt_custom_setup/combined_train_set.json'),
              mask_prompt=False,
              val_split_fraction=0.001,
              prompt_style=<litgpt.prompts.Alpaca object at 0x7f3fe0334e20>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=200,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True,
                  evaluate_example='first'),
 'logger_name': 'csv',
 'num_nodes': 1,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/data/michael_lu/poker_LLM/litgpt/full_finetune_out'),
 'precision': None,
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=500,
                    log_interval=50,
                    global_batch_size=8,
                    micro_batch_size=1,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=3,
                    max_tokens=None,
                    max_steps=None,
                    max_seq_length=None,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
[W1121 03:40:43.933650729 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
[W1121 03:40:43.937551223 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[W1121 03:40:43.993675985 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W1121 03:40:43.995453459 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

[rank: 2] Seed set to 1337
[rank: 0] Seed set to 1337
[rank: 1] Seed set to 1337
[rank: 3] Seed set to 1337
Number of trainable parameters: 3,606,752,256
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using torch.load with weights_only=False (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for weights_only will be flipped to True. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via torch.serialization.add_safe_globals. We recommend you start setting weights_only=True for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using torch.load with weights_only=False (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for weights_only will be flipped to True. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via torch.serialization.add_safe_globals. We recommend you start setting weights_only=True for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using torch.load with weights_only=False (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for weights_only will be flipped to True. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via torch.serialization.add_safe_globals. We recommend you start setting weights_only=True for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using torch.load with weights_only=False (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for weights_only will be flipped to True. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via torch.serialization.add_safe_globals. We recommend you start setting weights_only=True for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
The longest sequence length in the train data is 470, the model's maximum sequence length is 470 and context length is 131072
Verifying settings ...
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: torch.cpu.amp.autocast(args...) is deprecated. Please use torch.amp.autocast('cpu', args...) instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: torch.cpu.amp.autocast(args...) is deprecated. Please use torch.amp.autocast('cpu', args...) instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: torch.cpu.amp.autocast(args...) is deprecated. Please use torch.amp.autocast('cpu', args...) instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: torch.cpu.amp.autocast(args...) is deprecated. Please use torch.amp.autocast('cpu', args...) instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[rank2]: Traceback (most recent call last):
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/bin/litgpt", line 8, in <module>
[rank2]:     sys.exit(main())
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/__main__.py", line 71, in main
[rank2]:     CLI(parser_data)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank2]:     return _run_component(component, init.get(subcommand))
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank2]:     return component(**cfg)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 122, in setup
[rank2]:     fabric.launch(main, devices, resume, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank2]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank2]:     return to_run(*args, **kwargs)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank2]:     return to_run(*args, **kwargs)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 171, in main
[rank2]:     token_counts = fit(fabric, state, train_dataloader, val_dataloader, devices, resume, checkpoint_dir, out_dir, train, eval, data)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 267, in fit
[rank2]:     fabric.backward(loss / train.gradient_accumulation_iters(devices))
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 444, in backward
[rank2]:     self._strategy.backward(tensor, module, *args, **kwargs)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/strategy.py", line 188, in backward
[rank2]:     self.precision.backward(tensor, module, *args, **kwargs)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/plugins/precision/fsdp.py", line 128, in backward
[rank2]:     super().backward(tensor, model, *args, **kwargs)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/plugins/precision/precision.py", line 107, in backward
[rank2]:     tensor.backward(*args, **kwargs)
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.94 GiB. GPU 2 has a total capacity of 47.44 GiB of which 2.54 GiB is free.
Process 1485014 has 24.36 GiB memory in use. Including non-PyTorch memory, this process has 20.53 GiB memory in use. Of the allocated memory 19.91 GiB is allocated by PyTorch, and 137.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/bin/litgpt", line 8, in <module>
[rank0]:     sys.exit(main())
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/__main__.py", line 71, in main
[rank0]:     CLI(parser_data)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank0]:     return _run_component(component, init.get(subcommand))
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank0]:     return component(**cfg)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 122, in setup
[rank0]:     fabric.launch(main, devices, resume, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank0]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 928, in _wrap_and_launch
[rank0]:     return launcher.launch(to_run, *args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/launchers/subprocess_script.py", line 107, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank0]:     return to_run(*args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 171, in main
[rank0]:     token_counts = fit(fabric, state, train_dataloader, val_dataloader, devices, resume, checkpoint_dir, out_dir, train, eval, data)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 267, in fit
[rank0]:     fabric.backward(loss / train.gradient_accumulation_iters(devices))
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 444, in backward
[rank0]:     self._strategy.backward(tensor, module, *args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/strategy.py", line 188, in backward
[rank0]:     self.precision.backward(tensor, module, *args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/plugins/precision/fsdp.py", line 128, in backward
[rank0]:     super().backward(tensor, model, *args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/plugins/precision/precision.py", line 107, in backward
[rank0]:     tensor.backward(*args, **kwargs)
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.94 GiB. GPU 0 has a total capacity of 47.44 GiB of which 2.82 GiB is free. 
Process 1495295 has 24.04 GiB memory in use. Including non-PyTorch memory, this process has 20.56 GiB memory in use. Of the allocated memory 19.95 GiB is allocated by PyTorch, and 132.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/bin/litgpt", line 8, in <module>
[rank3]:     sys.exit(main())
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/__main__.py", line 71, in main
[rank3]:     CLI(parser_data)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/jsonargparse/_cli.py", line 119, in CLI
[rank3]:     return _run_component(component, init.get(subcommand))
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/jsonargparse/_cli.py", line 204, in _run_component
[rank3]:     return component(**cfg)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 122, in setup
[rank3]:     fabric.launch(main, devices, resume, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 843, in launch
[rank3]:     return self._wrap_and_launch(function, self, *args, **kwargs)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 929, in _wrap_and_launch
[rank3]:     return to_run(*args, **kwargs)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 934, in _wrap_with_setup
[rank3]:     return to_run(*args, **kwargs)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 171, in main
[rank3]:     token_counts = fit(fabric, state, train_dataloader, val_dataloader, devices, resume, checkpoint_dir, out_dir, train, eval, data)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/litgpt/finetune/full.py", line 267, in fit
[rank3]:     fabric.backward(loss / train.gradient_accumulation_iters(devices))
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/fabric.py", line 444, in backward
[rank3]:     self._strategy.backward(tensor, module, *args, **kwargs)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/strategies/strategy.py", line 188, in backward
[rank3]:     self.precision.backward(tensor, module, *args, **kwargs)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/plugins/precision/fsdp.py", line 128, in backward
[rank3]:     super().backward(tensor, model, *args, **kwargs)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/lightning/fabric/plugins/precision/precision.py", line 107, in backward
[rank3]:     tensor.backward(*args, **kwargs)
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.94 GiB. GPU 3 has a total capacity of 47.44 GiB of which 2.55 GiB is free. 
Process 1485775 has 24.36 GiB memory in use. Including non-PyTorch memory, this process has 20.51 GiB memory in use. Of the allocated memory 19.91 GiB is allocated by PyTorch, and 137.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(pokerenv) michael_lu@lingua:/data/michael_lu/poker_LLM/litgpt/checkpoints/meta-llama$ [rank1]:[E1121 04:22:53.748421077 ProcessGroupNCCL.cpp:607] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=_ALLGATHER_BASE, NumelIn=197001984, NumelOut=788007936, Timeout(ms)=1800000) ran for 1800070 milliseconds before timing out.
[rank1]:[E1121 04:22:53.749996533 ProcessGroupNCCL.cpp:1664] [PG 0 (default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 146, last enqueued NCCL work: 149, last completed NCCL work: 145.
[rank1]:[E1121 04:22:53.750030083 ProcessGroupNCCL.cpp:1709] [PG 0 (default_pg) Rank 1] Timeout at NCCL work: 146, last enqueued NCCL work: 149, last completed NCCL work: 145.
[rank1]:[E1121 04:22:53.750042613 ProcessGroupNCCL.cpp:621] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1121 04:22:53.750056602 ProcessGroupNCCL.cpp:627] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1121 04:22:53.759139331 ProcessGroupNCCL.cpp:1515] [PG 0 (default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=146, OpType=_ALLGATHER_BASE, NumelIn=197001984, NumelOut=788007936, Timeout(ms)=1800000) ran for 1800070 milliseconds before timing out.
Exception raised from checkTimeout at /opt/conda/conda-bld/pytorch_1724789122112/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:609 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f409a176f86 in /data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f404a22d0d2 in /data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f404a233b13 in /data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f404a235efc in /data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xdbbf4 (0x7f4099cc7bf4 in /data/michael_lu/anaconda3/envs/pokerenv/lib/python3.10/site-packages/torch/lib/../../../.././libstdc++.so.6)
frame #5: <unknown function> + 0x94ac3 (0x7f40aae8aac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7f40aaf1c850 in /lib/x86_64-linux-gnu/libc.so.6)